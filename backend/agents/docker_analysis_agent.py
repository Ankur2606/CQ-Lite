"""
Docker analysis agent for handling Docker file analysis in the workflow.
"""

import json
from typing import List, Dict, TypedDict
from backend.services.llm_service import get_llm_model
from backend.models.analysis_models import CodeIssue
from .state_schema import CodeAnalysisState
from backend.analyzers.docker_analyzer import DockerAnalyzer

def read_file_content(file_path: str, github_files: List[Dict] = None, max_chars: int = 2000) -> str:
    """
    Read file content with size limit, either from local file or GitHub data.
    
    Args:
        file_path: Path to the file
        github_files: Optional list of GitHub file dictionaries
        max_chars: Maximum number of characters to read
        
    Returns:
        File content or error message
    """
    # Check if we have GitHub files
    if github_files:
        from backend.analyzers.github_helpers import find_github_file_by_path
        github_file = find_github_file_by_path(github_files, file_path)
        if github_file:
            content = github_file.get("content", "")
            return content[:max_chars] + "..." if len(content) > max_chars else content
    
    # Fall back to local file
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            return content[:max_chars] + "..." if len(content) > max_chars else content
    except Exception:
        return "Could not read file content"

async def run_docker_analysis(file_path: str, github_files: List[Dict] = None) -> tuple[List[CodeIssue], any]:
    """
    Run Docker file analysis on a file, either from local path or GitHub repository.
    
    Args:
        file_path: Path to the file
        github_files: Optional list of GitHub file dictionaries
        
    Returns:
        Tuple of (issues, metrics)
    """
    analyzer = DockerAnalyzer()
    try:
        issues, metrics = await analyzer.analyze(file_path, github_files)
        return issues, metrics
    except Exception as e:
        print(f"‚ùå Error analyzing Docker file {file_path}: {e}")
        return [], None

def docker_analysis_agent(state: CodeAnalysisState) -> CodeAnalysisState:
    """AI-guided Docker file analysis with dynamic tool selection"""
    
    # Check for Dockerfiles in the discovered files
    # We need to add docker files to the discovery system
    docker_files = []
    
    # Look for files named 'Dockerfile' (no extension) or with .dockerfile extension
    all_files = []
    for file_list in state["discovered_files"].values():
        all_files.extend(file_list)
    
    # Filter for Docker files
    for file_path in all_files:
        filename = file_path.lower()
        if filename.endswith('dockerfile') or '/dockerfile' in filename or '\\dockerfile' in filename or '.dockerfile' in filename:
            docker_files.append(file_path)
    
    if not docker_files:
        print("üì¶ No Docker files found for analysis")
        return {
            **state, 
            "file_analysis_complete": {
                **state.get("file_analysis_complete", {}), 
                "docker": True
            }
        }
    
    # Get the selected model from the state
    model_choice = state.get("model_choice", "gemini")
    llm_model = get_llm_model(model_choice)

    docker_issues = []
    file_metadata = state.get("file_metadata", {})
    
    # Check for GitHub files in state
    github_files = state.get("github_files", [])

    print(f"üì¶ Analyzing {len(docker_files)} Docker files...")
    
    for file_path in docker_files:
        print(f"üìÅ Analyzing: {file_path}")
        # Traditional Analysis
        import asyncio
        issues, metrics = asyncio.run(run_docker_analysis(file_path, github_files))
        print(f"   Found {len(issues)} issues in {file_path}")
        
        # AI-Enhanced Analysis Decision Making
        file_content = read_file_content(file_path, github_files)
        analysis_prompt = f"""As a Docker and container security expert, analyze this Dockerfile and provide insights:

File: {file_path}
Issues Found: {len(issues)}
Code:
```dockerfile
{file_content}
```

Provide the following analysis:
1. Give a concise description of what this Dockerfile is building
2. Identify security vulnerabilities and their severity
3. Identify optimization opportunities to reduce image size and build time
4. Assess the use of best practices (multi-stage builds, layer reduction, etc.)
5. Provide specific improvement recommendations

IMPORTANT: You must respond with ONLY valid JSON. No additional text before or after.

Example response format:
{{
  "description": "Multi-stage Dockerfile that builds a Python web application with Alpine Linux.",
  "security_issues": [
    "HIGH: Container runs as root user, which is a security risk",
    "MEDIUM: Using pip without version pinning can lead to unexpected behaviors"
  ],
  "optimization_issues": [
    "Not using multi-stage builds increases image size",
    "Not combining RUN commands creates unnecessary layers"
  ],
  "best_practice_assessment": "Follows 4/10 best practices. Missing: multi-stage builds, layer reduction, .dockerignore, non-root user.",
  "recommendations": [
    "Add USER directive with non-root user",
    "Use multi-stage builds to separate build and runtime dependencies",
    "Combine RUN commands with && to reduce layers"
  ]
}}

Your response:"""
        try:
            if llm_model:
                print(f"   üß† Enhancing analysis with {model_choice}...")
                ai_decisions = llm_model.generate_content(analysis_prompt)
                docker_issues.extend(issues)
                
                # Parse AI analysis and add to file metadata
                try:
                    ai_content = ai_decisions.text.strip()
                    # Handle markdown formatting if present
                    if ai_content.startswith('```json'):
                        ai_content = ai_content[7:]
                    if ai_content.startswith('```'):
                        ai_content = ai_content[3:]
                    if ai_content.endswith('```'):
                        ai_content = ai_content[:-3]
                    
                    ai_content = ai_content.strip()
                    # Find JSON boundaries
                    start_idx = ai_content.find('{')
                    end_idx = ai_content.rfind('}')
                    
                    if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                        ai_metadata = json.loads(ai_content[start_idx:end_idx + 1])
                        file_metadata[file_path] = {
                            "description": ai_metadata.get("description", ""),
                            "security_issues": ai_metadata.get("security_issues", []),
                            "optimization_issues": ai_metadata.get("optimization_issues", []),
                            "best_practice_assessment": ai_metadata.get("best_practice_assessment", ""),
                            "recommendations": ai_metadata.get("recommendations", [])
                        }
                    else:
                        file_metadata[file_path] = {
                            "description": "Failed to parse AI analysis.",
                            "error": "Could not find valid JSON in AI response"
                        }
                except Exception as e:
                    print(f"   ‚ùå Error parsing AI analysis for {file_path}: {e}")
                    file_metadata[file_path] = {
                        "description": f"Error parsing AI analysis: {str(e)}",
                        "error": str(e)
                    }
            else:
                # Fallback to original issues if AI not available
                print(f"   ‚ö†Ô∏è No AI model available for enhancement. Using static analysis results.")
                docker_issues.extend(issues)
                file_metadata[file_path] = {
                    "description": "AI enhancement skipped.",
                    "error": "No AI model available"
                }
        except Exception as e:
            # Fallback to original issues if AI fails
            print(f"   ‚ùå AI enhancement failed for {file_path}: {e}")
            docker_issues.extend(issues)
            file_metadata[file_path] = {
                "description": f"AI enhancement failed: {e}",
                "error": str(e)
            }
    
    print(f"üì¶ Docker analysis complete: {len(docker_issues)} total issues found")
    
    return {
        **state,
        "docker_issues": docker_issues,
        "all_issues": state.get("all_issues", []) + docker_issues,
        "file_metadata": file_metadata,
        "file_analysis_complete": {
            **state.get("file_analysis_complete", {}), 
            "docker": True
        },
        "current_step": "docker_analysis_complete"
    }